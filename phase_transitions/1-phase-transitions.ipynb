{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter notebook for phase transitions during training\n",
    "\n",
    "1. Create local and global datasets\n",
    "2. Observe loss curves for phase transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global dataset\n",
    "\n",
    "We'll train a small transformer to iterate.\n",
    "\n",
    "*Note: the current setup of Prisma where train config + transformer config are in the same doc is super annoying.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New config \n",
    "Putting here instead of creating a config object for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.models.layers.transformer_block import TransformerBlock\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "PATCH_SIZE = 56 # 4 patches on width and on height, for a total of 16 patches\n",
    "\n",
    "@dataclass\n",
    "class ImageConfig:\n",
    "    image_size: int = IMAGE_SIZE\n",
    "    patch_size: int = PATCH_SIZE \n",
    "    n_channels: int = 1\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    hidden_dim: int = 128\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 1\n",
    "    block_fn = TransformerBlock\n",
    "    mlp_dim: int = hidden_dim * 4  # Use a computed default\n",
    "    activation_name: str = 'GELU'\n",
    "    attention_only: bool = False\n",
    "    attn_hidden_layer: bool = True\n",
    "\n",
    "@dataclass\n",
    "class LayerNormConfig:\n",
    "    qknorm: bool = False\n",
    "    layer_norm_eps: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class DropoutConfig:\n",
    "    patch: float = 0.0\n",
    "    position: float = 0.0\n",
    "    attention: float = 0.0\n",
    "    proj: float = 0.0\n",
    "    mlp: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class InitializationConfig:\n",
    "    weight_type: str = 'he'\n",
    "    cls_std: float = 1e-6\n",
    "    pos_std: float = 0.02\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    loss_fn_name: str = \"CrossEntropy\"\n",
    "    lr: float = 1e-4\n",
    "    num_epochs: int = 2000\n",
    "    batch_size: int = 256\n",
    "    warmup_steps: int = 0\n",
    "    weight_decay: float = 0.0\n",
    "    max_grad_norm = None\n",
    "    device: str = 'cuda'\n",
    "    seed: int = 0\n",
    "    optimizer_name: str = \"AdamW\"\n",
    "    scheduler_step: int = num_epochs\n",
    "    scheduler_gamma: float = .8\n",
    "    early_stopping: bool = False\n",
    "\n",
    "@dataclass\n",
    "class LoggingConfig:\n",
    "    log_dir: str = 'logs'\n",
    "    log_frequency: int = 1\n",
    "    print_every: int = 0\n",
    "    use_wandb: bool = True\n",
    "    wandb_project_name: str = 'phase_transitions'  # Added type annotation\n",
    "    wandb_team_name: str = 'perceptual-alignment'\n",
    "\n",
    "@dataclass\n",
    "class SavingConfig:\n",
    "    parent_dir: str = \"/network/scratch/s/sonia.joseph/vit_prisma\"\n",
    "    save_dir: str = 'checkpoints'\n",
    "    save_checkpoints: bool = True\n",
    "    save_cp_frequency: int = 1\n",
    "\n",
    "class ClassificationConfig:\n",
    "    num_classes: int = 16\n",
    "    global_pool: bool = False\n",
    "\n",
    "@dataclass\n",
    "class GlobalConfig():\n",
    "    image: ImageConfig = ImageConfig()\n",
    "    transformer: TransformerConfig = TransformerConfig()\n",
    "    layernorm: LayerNormConfig = LayerNormConfig()\n",
    "    dropout: DropoutConfig = DropoutConfig()\n",
    "    init: InitializationConfig = InitializationConfig()\n",
    "    training: TrainingConfig = TrainingConfig()\n",
    "    logging: LoggingConfig = LoggingConfig()\n",
    "    saving: SavingConfig = SavingConfig()\n",
    "    classification: ClassificationConfig = ClassificationConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global dataset\n",
    "\n",
    " Color in random number of patches. The label for the dataset is the number of patches colored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "from torchvision import transforms\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, total_images, image_size=28, num_patches=16, transform=None):\n",
    "        # Validate inputs\n",
    "        if not np.sqrt(num_patches).is_integer():\n",
    "            raise ValueError(\"num_patches must be a perfect square\")\n",
    "        if image_size % np.sqrt(num_patches) != 0:\n",
    "            raise ValueError(\"image_size must be divisible by the square root of num_patches\")\n",
    "\n",
    "        self.total_images = total_images\n",
    "        self.image_size = image_size\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = image_size // int(np.sqrt(num_patches))\n",
    "        self.transform = transform\n",
    "\n",
    "        self.range_start = 6\n",
    "        self.range_end = 12\n",
    "\n",
    "        # Generate all unique combinations\n",
    "        self.all_combinations = list(itertools.chain.from_iterable(\n",
    "            itertools.combinations(range(num_patches), r) for r in range(self.range_start, self.range_end)\n",
    "        ))\n",
    "\n",
    "        if self.total_images > len(self.all_combinations):\n",
    "            raise ValueError(\"Requested total images exceed the number of unique combinations available\")\n",
    "\n",
    "        # Shuffle to randomize\n",
    "        np.random.shuffle(self.all_combinations)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select combination based on index\n",
    "        selected_patches = self.all_combinations[idx]\n",
    "        image = self._generate_image(selected_patches)\n",
    "        label = len(selected_patches)  # Class label is the number of patches\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _generate_image(self, selected_patches):\n",
    "        image_array = np.zeros((self.image_size, self.image_size), dtype=np.uint8)\n",
    "        num_patch_side = int(np.sqrt(self.num_patches))\n",
    "\n",
    "        for patch_index in selected_patches:\n",
    "            row, col = divmod(patch_index, num_patch_side)\n",
    "            image_array[row * self.patch_size:(row + 1) * self.patch_size, col * self.patch_size:(col + 1) * self.patch_size] = 255\n",
    "\n",
    "        return Image.fromarray(image_array, 'L')\n",
    "\n",
    "# Example usage\n",
    "total_images = 10000  # Adjust as needed\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "patch_dataset = PatchDataset(total_images=total_images, image_size=IMAGE_SIZE, transform=transform)\n",
    "\n",
    "# Balance dataset to have number of classes as smallest class\n",
    "\n",
    "# Initialize a Counter to count class occurrences\n",
    "class_counts = Counter()\n",
    "# Iterate over the dataset and count classes\n",
    "for _, label in patch_dataset:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "min_class_size = min(class_counts.values())\n",
    "\n",
    "\n",
    "balanced_global_dataset = []\n",
    "\n",
    "# Iterate over each class and add a random sample of images to the balanced dataset\n",
    "for class_label in range(patch_dataset.range_start, patch_dataset.range_end):\n",
    "    # Filter images of the current class\n",
    "    class_images = [(image, label) for image, label in patch_dataset if label == class_label]\n",
    "    \n",
    "    # Randomly sample images from this class\n",
    "    balanced_images = random.sample(class_images, min_class_size)\n",
    "    \n",
    "    balanced_global_dataset.extend(balanced_images)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "random.shuffle(balanced_global_dataset)\n",
    "\n",
    "# Now balanced_global_dataset contains a balanced number of images for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({8: 4368, 11: 4368, 9: 4368, 7: 4368, 10: 4368, 6: 4368})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = Counter()\n",
    "for _, label in balanced_global_dataset:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import transform\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Plotting the images in a line\n",
    "num_images=20\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(20, 2))\n",
    "\n",
    "for i in range(num_images):\n",
    "    custom_image, custom_label = balanced_global_dataset[i]\n",
    "    axes[i].imshow(custom_image[0], cmap='gray')\n",
    "    axes[i].set_title(f'Label: {custom_label}')\n",
    "    axes[i].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1120, 3: 1058, 7: 1052, 9: 1011, 2: 1006, 8: 999, 6: 982, 0: 955, 4: 926, 5: 891})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "class LocalPatchDataset(Dataset):\n",
    "    def __init__(self, total_images, mnist_dataset, image_size=28, num_patches=16, transform=None):\n",
    "        self.total_images = total_images\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.image_size = image_size\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = image_size // int(np.sqrt(num_patches))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self._generate_image()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _generate_image(self):\n",
    "        image_array = np.zeros((self.image_size, self.image_size), dtype=np.uint8)\n",
    "        num_patch_side = int(np.sqrt(self.num_patches))\n",
    "\n",
    "        # Randomly choose a patch to place an MNIST digit\n",
    "        # Randomly choose a patch to place an MNIST digit\n",
    "        mnist_index = random.randint(0, len(self.mnist_dataset) - 1)\n",
    "        mnist_digit, label = self.mnist_dataset[mnist_index]\n",
    "        mnist_digit = np.array(mnist_digit) * 255  # Rescale to 0-255 and convert to uint8\n",
    "        mnist_digit = mnist_digit.astype(np.uint8)\n",
    "\n",
    "        patch_x, patch_y = random.randint(0, num_patch_side - 1), random.randint(0, num_patch_side - 1)\n",
    "        x_start, y_start = patch_x * self.patch_size, patch_y * self.patch_size\n",
    "\n",
    "        # Place the MNIST digit into the selected patch\n",
    "        image_array[x_start:x_start+self.patch_size, y_start:y_start+self.patch_size] = mnist_digit\n",
    "\n",
    "        return Image.fromarray(image_array, 'L'), label\n",
    "\n",
    "# Load MNIST Dataset\n",
    "mnist_transform = transforms.Compose([transforms.Resize((56, 56)), transforms.ToTensor()])\n",
    "# add to transforms to rescale pixel values to \n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "\n",
    "# Example usage\n",
    "local_patch_dataset = LocalPatchDataset(total_images=total_images, mnist_dataset=mnist_dataset, image_size=224, transform=transform)\n",
    "\n",
    "# Count classes\n",
    "class_counts = Counter()\n",
    "for i in range(total_images):\n",
    "    label = local_patch_dataset[i][1]\n",
    "    class_counts[label] += 1\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    image, label = local_patch_dataset[i]\n",
    "    axes[i].imshow(image[0], cmap='gray')\n",
    "    axes[i].set_title(f'Label: {label}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train dataset into train and val with 8000 and 2000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoniajoseph\u001b[0m (\u001b[33mperceptual-alignment\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/s/sonia.joseph/ViT-Planetarium/my_draft/phase_transitions/wandb/run-20240109_174342-yjs5sfq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/perceptual-alignment/phase_transitions/runs/yjs5sfq9' target=\"_blank\">denim-durian-13</a></strong> to <a href='https://wandb.ai/perceptual-alignment/phase_transitions' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/perceptual-alignment/phase_transitions' target=\"_blank\">https://wandb.ai/perceptual-alignment/phase_transitions</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/perceptual-alignment/phase_transitions/runs/yjs5sfq9' target=\"_blank\">https://wandb.ai/perceptual-alignment/phase_transitions/runs/yjs5sfq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config is: GlobalConfig(image=ImageConfig(image_size=224, patch_size=56, n_channels=1), transformer=TransformerConfig(hidden_dim=128, num_heads=4, num_layers=1, mlp_dim=512, activation_name='GELU', attention_only=False, attn_hidden_layer=True), layernorm=LayerNormConfig(qknorm=False, layer_norm_eps=0.0), dropout=DropoutConfig(patch=0.0, position=0.0, attention=0.0, proj=0.0, mlp=0.0), init=InitializationConfig(weight_type='he', cls_std=1e-06, pos_std=0.02), training=TrainingConfig(loss_fn_name='CrossEntropy', lr=0.0001, num_epochs=2000, batch_size=256, warmup_steps=0, weight_decay=0.0, device='cuda', seed=0, optimizer_name='AdamW', scheduler_step=2000, scheduler_gamma=0.8, early_stopping=False), logging=LoggingConfig(log_dir='logs', log_frequency=1, print_every=0, use_wandb=True, wandb_project_name='phase_transitions', wandb_team_name='perceptual-alignment'), saving=SavingConfig(parent_dir='/network/scratch/s/sonia.joseph/vit_prisma', save_dir='checkpoints', save_checkpoints=True, save_cp_frequency=1), classification=<__main__.ClassificationConfig object at 0x7f88d2630390>)\n",
      "Length of trainloader 32.\n",
      "Length of testloader 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdb7bb3cfdd46b392466ba8708ce80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps0 | Train loss: 0.011111 | Train acc: 0.12012 | Test loss: 0.011117 | Test acc: 0.11400\n",
      "Steps1 | Train loss: 0.011008 | Train acc: 0.18288 | Test loss: 0.011016 | Test acc: 0.17450\n",
      "Steps2 | Train loss: 0.010905 | Train acc: 0.26550 | Test loss: 0.010915 | Test acc: 0.24450\n",
      "Steps3 | Train loss: 0.010804 | Train acc: 0.30075 | Test loss: 0.010815 | Test acc: 0.27800\n",
      "Steps4 | Train loss: 0.010705 | Train acc: 0.32500 | Test loss: 0.010716 | Test acc: 0.30500\n",
      "Steps5 | Train loss: 0.010601 | Train acc: 0.34763 | Test loss: 0.010615 | Test acc: 0.33000\n",
      "Steps6 | Train loss: 0.010497 | Train acc: 0.36412 | Test loss: 0.010514 | Test acc: 0.34500\n",
      "Steps7 | Train loss: 0.010395 | Train acc: 0.37975 | Test loss: 0.010411 | Test acc: 0.35850\n",
      "Steps8 | Train loss: 0.010286 | Train acc: 0.38812 | Test loss: 0.010306 | Test acc: 0.36950\n",
      "Steps9 | Train loss: 0.010177 | Train acc: 0.39663 | Test loss: 0.010199 | Test acc: 0.36950\n",
      "Steps10 | Train loss: 0.010064 | Train acc: 0.39850 | Test loss: 0.010089 | Test acc: 0.37650\n",
      "Steps11 | Train loss: 0.009953 | Train acc: 0.40450 | Test loss: 0.009977 | Test acc: 0.38500\n",
      "Steps12 | Train loss: 0.009834 | Train acc: 0.39937 | Test loss: 0.009862 | Test acc: 0.38600\n",
      "Steps13 | Train loss: 0.009716 | Train acc: 0.39075 | Test loss: 0.009746 | Test acc: 0.37200\n",
      "Steps14 | Train loss: 0.009598 | Train acc: 0.39337 | Test loss: 0.009628 | Test acc: 0.37650\n",
      "Steps15 | Train loss: 0.009474 | Train acc: 0.39625 | Test loss: 0.009507 | Test acc: 0.38300\n",
      "Steps16 | Train loss: 0.009354 | Train acc: 0.39325 | Test loss: 0.009383 | Test acc: 0.38400\n",
      "Steps17 | Train loss: 0.009216 | Train acc: 0.39675 | Test loss: 0.009258 | Test acc: 0.38600\n",
      "Steps18 | Train loss: 0.009089 | Train acc: 0.40450 | Test loss: 0.009129 | Test acc: 0.39000\n",
      "Steps19 | Train loss: 0.008961 | Train acc: 0.41287 | Test loss: 0.008998 | Test acc: 0.40150\n",
      "Steps20 | Train loss: 0.008823 | Train acc: 0.41750 | Test loss: 0.008866 | Test acc: 0.40900\n",
      "Steps21 | Train loss: 0.008690 | Train acc: 0.41375 | Test loss: 0.008733 | Test acc: 0.40400\n",
      "Steps22 | Train loss: 0.008549 | Train acc: 0.41675 | Test loss: 0.008596 | Test acc: 0.41400\n",
      "Steps23 | Train loss: 0.008406 | Train acc: 0.42938 | Test loss: 0.008457 | Test acc: 0.42850\n",
      "Steps24 | Train loss: 0.008266 | Train acc: 0.45150 | Test loss: 0.008314 | Test acc: 0.44850\n",
      "Steps25 | Train loss: 0.008110 | Train acc: 0.47900 | Test loss: 0.008167 | Test acc: 0.47350\n",
      "Steps26 | Train loss: 0.007963 | Train acc: 0.50625 | Test loss: 0.008019 | Test acc: 0.50200\n",
      "Steps27 | Train loss: 0.007807 | Train acc: 0.53587 | Test loss: 0.007868 | Test acc: 0.52450\n",
      "Steps28 | Train loss: 0.007664 | Train acc: 0.56325 | Test loss: 0.007714 | Test acc: 0.54950\n",
      "Steps29 | Train loss: 0.007502 | Train acc: 0.59350 | Test loss: 0.007559 | Test acc: 0.58000\n",
      "Steps30 | Train loss: 0.007348 | Train acc: 0.61038 | Test loss: 0.007404 | Test acc: 0.60050\n",
      "Steps31 | Train loss: 0.007195 | Train acc: 0.62450 | Test loss: 0.007250 | Test acc: 0.62000\n",
      "Steps32 | Train loss: 0.007039 | Train acc: 0.63375 | Test loss: 0.007099 | Test acc: 0.63100\n",
      "Steps33 | Train loss: 0.006886 | Train acc: 0.64187 | Test loss: 0.006948 | Test acc: 0.63600\n",
      "Steps34 | Train loss: 0.006740 | Train acc: 0.64912 | Test loss: 0.006795 | Test acc: 0.64150\n",
      "Steps35 | Train loss: 0.006564 | Train acc: 0.65500 | Test loss: 0.006642 | Test acc: 0.65050\n",
      "Steps36 | Train loss: 0.006420 | Train acc: 0.65763 | Test loss: 0.006491 | Test acc: 0.65050\n",
      "Steps37 | Train loss: 0.006283 | Train acc: 0.66038 | Test loss: 0.006340 | Test acc: 0.65400\n",
      "Steps38 | Train loss: 0.006118 | Train acc: 0.66338 | Test loss: 0.006189 | Test acc: 0.65700\n",
      "Steps39 | Train loss: 0.005970 | Train acc: 0.66187 | Test loss: 0.006040 | Test acc: 0.65850\n",
      "Steps40 | Train loss: 0.005812 | Train acc: 0.66638 | Test loss: 0.005890 | Test acc: 0.66100\n",
      "Steps41 | Train loss: 0.005661 | Train acc: 0.67437 | Test loss: 0.005741 | Test acc: 0.66750\n",
      "Steps42 | Train loss: 0.005513 | Train acc: 0.67863 | Test loss: 0.005594 | Test acc: 0.67000\n",
      "Steps43 | Train loss: 0.005369 | Train acc: 0.68763 | Test loss: 0.005447 | Test acc: 0.67600\n",
      "Steps44 | Train loss: 0.005219 | Train acc: 0.69563 | Test loss: 0.005304 | Test acc: 0.68250\n",
      "Steps45 | Train loss: 0.005070 | Train acc: 0.70050 | Test loss: 0.005165 | Test acc: 0.68850\n",
      "Steps46 | Train loss: 0.004934 | Train acc: 0.70800 | Test loss: 0.005029 | Test acc: 0.69350\n",
      "Steps47 | Train loss: 0.004798 | Train acc: 0.71388 | Test loss: 0.004896 | Test acc: 0.69950\n",
      "Steps48 | Train loss: 0.004660 | Train acc: 0.71737 | Test loss: 0.004767 | Test acc: 0.70600\n",
      "Steps49 | Train loss: 0.004536 | Train acc: 0.71925 | Test loss: 0.004641 | Test acc: 0.70750\n",
      "Steps50 | Train loss: 0.004425 | Train acc: 0.71950 | Test loss: 0.004516 | Test acc: 0.71500\n",
      "Steps51 | Train loss: 0.004271 | Train acc: 0.72225 | Test loss: 0.004395 | Test acc: 0.71200\n",
      "Steps52 | Train loss: 0.004162 | Train acc: 0.72588 | Test loss: 0.004275 | Test acc: 0.71600\n",
      "Steps53 | Train loss: 0.004042 | Train acc: 0.73250 | Test loss: 0.004158 | Test acc: 0.72150\n",
      "Steps54 | Train loss: 0.003943 | Train acc: 0.74125 | Test loss: 0.004042 | Test acc: 0.72800\n",
      "Steps55 | Train loss: 0.003813 | Train acc: 0.74462 | Test loss: 0.003933 | Test acc: 0.72950\n",
      "Steps56 | Train loss: 0.003706 | Train acc: 0.74787 | Test loss: 0.003827 | Test acc: 0.73600\n",
      "Steps57 | Train loss: 0.003618 | Train acc: 0.75675 | Test loss: 0.003720 | Test acc: 0.74050\n",
      "Steps58 | Train loss: 0.003518 | Train acc: 0.76850 | Test loss: 0.003617 | Test acc: 0.75200\n",
      "Steps59 | Train loss: 0.003425 | Train acc: 0.77612 | Test loss: 0.003521 | Test acc: 0.76000\n",
      "Steps60 | Train loss: 0.003333 | Train acc: 0.78100 | Test loss: 0.003428 | Test acc: 0.76450\n",
      "Steps61 | Train loss: 0.003248 | Train acc: 0.78787 | Test loss: 0.003340 | Test acc: 0.76550\n",
      "Steps62 | Train loss: 0.003162 | Train acc: 0.79275 | Test loss: 0.003255 | Test acc: 0.77550\n",
      "Steps63 | Train loss: 0.003076 | Train acc: 0.79587 | Test loss: 0.003173 | Test acc: 0.77950\n",
      "Steps64 | Train loss: 0.002992 | Train acc: 0.80000 | Test loss: 0.003104 | Test acc: 0.77450\n",
      "Steps65 | Train loss: 0.002951 | Train acc: 0.79988 | Test loss: 0.003050 | Test acc: 0.77250\n",
      "Steps66 | Train loss: 0.002885 | Train acc: 0.80075 | Test loss: 0.002993 | Test acc: 0.77250\n",
      "Steps67 | Train loss: 0.002836 | Train acc: 0.80100 | Test loss: 0.002938 | Test acc: 0.77800\n",
      "Steps68 | Train loss: 0.002769 | Train acc: 0.80525 | Test loss: 0.002875 | Test acc: 0.78500\n",
      "Steps69 | Train loss: 0.002708 | Train acc: 0.81100 | Test loss: 0.002801 | Test acc: 0.79200\n",
      "Steps70 | Train loss: 0.002654 | Train acc: 0.81388 | Test loss: 0.002731 | Test acc: 0.80250\n",
      "Steps71 | Train loss: 0.002590 | Train acc: 0.81625 | Test loss: 0.002675 | Test acc: 0.80350\n",
      "Steps72 | Train loss: 0.002538 | Train acc: 0.81725 | Test loss: 0.002620 | Test acc: 0.80350\n",
      "Steps73 | Train loss: 0.002486 | Train acc: 0.82250 | Test loss: 0.002568 | Test acc: 0.80700\n",
      "Steps74 | Train loss: 0.002449 | Train acc: 0.82350 | Test loss: 0.002528 | Test acc: 0.81200\n",
      "Steps75 | Train loss: 0.002408 | Train acc: 0.82650 | Test loss: 0.002497 | Test acc: 0.81400\n",
      "Steps76 | Train loss: 0.002350 | Train acc: 0.83088 | Test loss: 0.002462 | Test acc: 0.81600\n",
      "Steps77 | Train loss: 0.002321 | Train acc: 0.83300 | Test loss: 0.002435 | Test acc: 0.81150\n",
      "Steps78 | Train loss: 0.002297 | Train acc: 0.83488 | Test loss: 0.002403 | Test acc: 0.81350\n",
      "Steps79 | Train loss: 0.002272 | Train acc: 0.83762 | Test loss: 0.002370 | Test acc: 0.81900\n",
      "Steps80 | Train loss: 0.002253 | Train acc: 0.84200 | Test loss: 0.002327 | Test acc: 0.82950\n",
      "Steps81 | Train loss: 0.002195 | Train acc: 0.84450 | Test loss: 0.002289 | Test acc: 0.83400\n",
      "Steps82 | Train loss: 0.002172 | Train acc: 0.84662 | Test loss: 0.002250 | Test acc: 0.83650\n",
      "Steps83 | Train loss: 0.002158 | Train acc: 0.84775 | Test loss: 0.002212 | Test acc: 0.84250\n",
      "Steps84 | Train loss: 0.002105 | Train acc: 0.84900 | Test loss: 0.002175 | Test acc: 0.84100\n",
      "Steps85 | Train loss: 0.002080 | Train acc: 0.85100 | Test loss: 0.002138 | Test acc: 0.84350\n",
      "Steps86 | Train loss: 0.002040 | Train acc: 0.85437 | Test loss: 0.002106 | Test acc: 0.84550\n",
      "Steps87 | Train loss: 0.002007 | Train acc: 0.85637 | Test loss: 0.002073 | Test acc: 0.84400\n",
      "Steps88 | Train loss: 0.002005 | Train acc: 0.85550 | Test loss: 0.002059 | Test acc: 0.84800\n",
      "Steps89 | Train loss: 0.001991 | Train acc: 0.85362 | Test loss: 0.002058 | Test acc: 0.84250\n",
      "Steps90 | Train loss: 0.001983 | Train acc: 0.85688 | Test loss: 0.002027 | Test acc: 0.84850\n",
      "Steps91 | Train loss: 0.001933 | Train acc: 0.86100 | Test loss: 0.001993 | Test acc: 0.85950\n",
      "Steps92 | Train loss: 0.001908 | Train acc: 0.86587 | Test loss: 0.001953 | Test acc: 0.86200\n",
      "Steps93 | Train loss: 0.001883 | Train acc: 0.86925 | Test loss: 0.001916 | Test acc: 0.87050\n",
      "Steps94 | Train loss: 0.001876 | Train acc: 0.87262 | Test loss: 0.001903 | Test acc: 0.86950\n",
      "Steps95 | Train loss: 0.001854 | Train acc: 0.87275 | Test loss: 0.001899 | Test acc: 0.86500\n",
      "Steps96 | Train loss: 0.001855 | Train acc: 0.87200 | Test loss: 0.001902 | Test acc: 0.86300\n",
      "Steps97 | Train loss: 0.001840 | Train acc: 0.87687 | Test loss: 0.001899 | Test acc: 0.86800\n",
      "Steps98 | Train loss: 0.001811 | Train acc: 0.87762 | Test loss: 0.001902 | Test acc: 0.87200\n",
      "Steps99 | Train loss: 0.001809 | Train acc: 0.87738 | Test loss: 0.001911 | Test acc: 0.87050\n",
      "Steps100 | Train loss: 0.001765 | Train acc: 0.87938 | Test loss: 0.001891 | Test acc: 0.86800\n",
      "Steps101 | Train loss: 0.001756 | Train acc: 0.87625 | Test loss: 0.001876 | Test acc: 0.86500\n",
      "Steps102 | Train loss: 0.001741 | Train acc: 0.87438 | Test loss: 0.001867 | Test acc: 0.86450\n",
      "Steps103 | Train loss: 0.001733 | Train acc: 0.87438 | Test loss: 0.001857 | Test acc: 0.86250\n",
      "Steps104 | Train loss: 0.001751 | Train acc: 0.87562 | Test loss: 0.001837 | Test acc: 0.86600\n",
      "Steps105 | Train loss: 0.001744 | Train acc: 0.87575 | Test loss: 0.001809 | Test acc: 0.86950\n",
      "Steps106 | Train loss: 0.001714 | Train acc: 0.87950 | Test loss: 0.001780 | Test acc: 0.87000\n",
      "Steps107 | Train loss: 0.001694 | Train acc: 0.88275 | Test loss: 0.001759 | Test acc: 0.87500\n",
      "Steps108 | Train loss: 0.001671 | Train acc: 0.88450 | Test loss: 0.001741 | Test acc: 0.87850\n",
      "Steps109 | Train loss: 0.001669 | Train acc: 0.88338 | Test loss: 0.001742 | Test acc: 0.87700\n",
      "Steps110 | Train loss: 0.001675 | Train acc: 0.88362 | Test loss: 0.001757 | Test acc: 0.87650\n",
      "Steps111 | Train loss: 0.001661 | Train acc: 0.88400 | Test loss: 0.001765 | Test acc: 0.87650\n",
      "Steps112 | Train loss: 0.001659 | Train acc: 0.88387 | Test loss: 0.001758 | Test acc: 0.87600\n",
      "Steps113 | Train loss: 0.001641 | Train acc: 0.88438 | Test loss: 0.001749 | Test acc: 0.87300\n",
      "Steps114 | Train loss: 0.001612 | Train acc: 0.88550 | Test loss: 0.001740 | Test acc: 0.87450\n",
      "Steps115 | Train loss: 0.001604 | Train acc: 0.88875 | Test loss: 0.001718 | Test acc: 0.87800\n",
      "Steps116 | Train loss: 0.001549 | Train acc: 0.89150 | Test loss: 0.001685 | Test acc: 0.88050\n",
      "Steps117 | Train loss: 0.001552 | Train acc: 0.89238 | Test loss: 0.001672 | Test acc: 0.88300\n",
      "Steps118 | Train loss: 0.001578 | Train acc: 0.88913 | Test loss: 0.001681 | Test acc: 0.87950\n",
      "Steps119 | Train loss: 0.001617 | Train acc: 0.88513 | Test loss: 0.001708 | Test acc: 0.87650\n",
      "Steps120 | Train loss: 0.001637 | Train acc: 0.88300 | Test loss: 0.001720 | Test acc: 0.87300\n",
      "Steps121 | Train loss: 0.001599 | Train acc: 0.88675 | Test loss: 0.001691 | Test acc: 0.87750\n",
      "Steps122 | Train loss: 0.001558 | Train acc: 0.89312 | Test loss: 0.001649 | Test acc: 0.88300\n",
      "Steps123 | Train loss: 0.001546 | Train acc: 0.89700 | Test loss: 0.001620 | Test acc: 0.88900\n",
      "Steps124 | Train loss: 0.001501 | Train acc: 0.89950 | Test loss: 0.001620 | Test acc: 0.88800\n",
      "Steps125 | Train loss: 0.001514 | Train acc: 0.89812 | Test loss: 0.001642 | Test acc: 0.88900\n",
      "Steps126 | Train loss: 0.001543 | Train acc: 0.89412 | Test loss: 0.001671 | Test acc: 0.88250\n",
      "Steps127 | Train loss: 0.001552 | Train acc: 0.89438 | Test loss: 0.001672 | Test acc: 0.88100\n",
      "Steps128 | Train loss: 0.001536 | Train acc: 0.89375 | Test loss: 0.001668 | Test acc: 0.87900\n",
      "Steps129 | Train loss: 0.001555 | Train acc: 0.89087 | Test loss: 0.001692 | Test acc: 0.87900\n",
      "Steps130 | Train loss: 0.001564 | Train acc: 0.89000 | Test loss: 0.001707 | Test acc: 0.87700\n",
      "Steps131 | Train loss: 0.001576 | Train acc: 0.88638 | Test loss: 0.001728 | Test acc: 0.87350\n",
      "Steps132 | Train loss: 0.001601 | Train acc: 0.88613 | Test loss: 0.001731 | Test acc: 0.87250\n",
      "Steps133 | Train loss: 0.001572 | Train acc: 0.88875 | Test loss: 0.001702 | Test acc: 0.87850\n",
      "Steps134 | Train loss: 0.001496 | Train acc: 0.89338 | Test loss: 0.001636 | Test acc: 0.88050\n",
      "Steps135 | Train loss: 0.001452 | Train acc: 0.89587 | Test loss: 0.001601 | Test acc: 0.88600\n",
      "Steps136 | Train loss: 0.001464 | Train acc: 0.89775 | Test loss: 0.001602 | Test acc: 0.88600\n",
      "Steps137 | Train loss: 0.001492 | Train acc: 0.89550 | Test loss: 0.001628 | Test acc: 0.88000\n",
      "Steps138 | Train loss: 0.001485 | Train acc: 0.89312 | Test loss: 0.001643 | Test acc: 0.87700\n",
      "Steps139 | Train loss: 0.001479 | Train acc: 0.89387 | Test loss: 0.001642 | Test acc: 0.87650\n",
      "Steps140 | Train loss: 0.001479 | Train acc: 0.89363 | Test loss: 0.001640 | Test acc: 0.87800\n",
      "Steps141 | Train loss: 0.001458 | Train acc: 0.89450 | Test loss: 0.001634 | Test acc: 0.88300\n",
      "Steps142 | Train loss: 0.001467 | Train acc: 0.89700 | Test loss: 0.001617 | Test acc: 0.88350\n",
      "Steps143 | Train loss: 0.001457 | Train acc: 0.89738 | Test loss: 0.001603 | Test acc: 0.88550\n",
      "Steps144 | Train loss: 0.001435 | Train acc: 0.90012 | Test loss: 0.001585 | Test acc: 0.88850\n",
      "Steps145 | Train loss: 0.001453 | Train acc: 0.90063 | Test loss: 0.001574 | Test acc: 0.88800\n",
      "Steps146 | Train loss: 0.001477 | Train acc: 0.89787 | Test loss: 0.001575 | Test acc: 0.88900\n",
      "Steps147 | Train loss: 0.001449 | Train acc: 0.89725 | Test loss: 0.001569 | Test acc: 0.88800\n",
      "Steps148 | Train loss: 0.001410 | Train acc: 0.90275 | Test loss: 0.001529 | Test acc: 0.89300\n",
      "Steps149 | Train loss: 0.001351 | Train acc: 0.90438 | Test loss: 0.001501 | Test acc: 0.89500\n",
      "Steps150 | Train loss: 0.001357 | Train acc: 0.90600 | Test loss: 0.001494 | Test acc: 0.89550\n",
      "Steps151 | Train loss: 0.001356 | Train acc: 0.90287 | Test loss: 0.001507 | Test acc: 0.89550\n",
      "Steps152 | Train loss: 0.001358 | Train acc: 0.90225 | Test loss: 0.001524 | Test acc: 0.89150\n",
      "Steps153 | Train loss: 0.001385 | Train acc: 0.90125 | Test loss: 0.001541 | Test acc: 0.88600\n",
      "Steps154 | Train loss: 0.001374 | Train acc: 0.90187 | Test loss: 0.001545 | Test acc: 0.88500\n",
      "Steps155 | Train loss: 0.001386 | Train acc: 0.90263 | Test loss: 0.001548 | Test acc: 0.88700\n",
      "Steps156 | Train loss: 0.001377 | Train acc: 0.90212 | Test loss: 0.001541 | Test acc: 0.88550\n",
      "Steps157 | Train loss: 0.001342 | Train acc: 0.90450 | Test loss: 0.001523 | Test acc: 0.88850\n",
      "Steps158 | Train loss: 0.001319 | Train acc: 0.90587 | Test loss: 0.001503 | Test acc: 0.89250\n",
      "Steps159 | Train loss: 0.001327 | Train acc: 0.90900 | Test loss: 0.001486 | Test acc: 0.89450\n",
      "Steps160 | Train loss: 0.001282 | Train acc: 0.91138 | Test loss: 0.001482 | Test acc: 0.89550\n",
      "Steps161 | Train loss: 0.001310 | Train acc: 0.91150 | Test loss: 0.001480 | Test acc: 0.89550\n",
      "Steps162 | Train loss: 0.001295 | Train acc: 0.91138 | Test loss: 0.001474 | Test acc: 0.89800\n",
      "Steps163 | Train loss: 0.001291 | Train acc: 0.91150 | Test loss: 0.001459 | Test acc: 0.89950\n",
      "Steps164 | Train loss: 0.001285 | Train acc: 0.91100 | Test loss: 0.001450 | Test acc: 0.90050\n",
      "Steps165 | Train loss: 0.001275 | Train acc: 0.91138 | Test loss: 0.001438 | Test acc: 0.90150\n",
      "Steps166 | Train loss: 0.001272 | Train acc: 0.91075 | Test loss: 0.001432 | Test acc: 0.90100\n",
      "Steps167 | Train loss: 0.001284 | Train acc: 0.91100 | Test loss: 0.001430 | Test acc: 0.89900\n",
      "Steps168 | Train loss: 0.001270 | Train acc: 0.91200 | Test loss: 0.001429 | Test acc: 0.89800\n",
      "Steps169 | Train loss: 0.001274 | Train acc: 0.91375 | Test loss: 0.001433 | Test acc: 0.89750\n",
      "Steps170 | Train loss: 0.001275 | Train acc: 0.91375 | Test loss: 0.001434 | Test acc: 0.89550\n",
      "Steps171 | Train loss: 0.001276 | Train acc: 0.91450 | Test loss: 0.001440 | Test acc: 0.89500\n",
      "Steps172 | Train loss: 0.001260 | Train acc: 0.91487 | Test loss: 0.001439 | Test acc: 0.89600\n",
      "Steps173 | Train loss: 0.001255 | Train acc: 0.91600 | Test loss: 0.001435 | Test acc: 0.90200\n",
      "Steps174 | Train loss: 0.001236 | Train acc: 0.91625 | Test loss: 0.001442 | Test acc: 0.89750\n",
      "Steps175 | Train loss: 0.001234 | Train acc: 0.91612 | Test loss: 0.001453 | Test acc: 0.89950\n",
      "Steps176 | Train loss: 0.001269 | Train acc: 0.91275 | Test loss: 0.001486 | Test acc: 0.89500\n",
      "Steps177 | Train loss: 0.001293 | Train acc: 0.90763 | Test loss: 0.001521 | Test acc: 0.89000\n",
      "Steps178 | Train loss: 0.001293 | Train acc: 0.90787 | Test loss: 0.001517 | Test acc: 0.89100\n",
      "Steps179 | Train loss: 0.001269 | Train acc: 0.91063 | Test loss: 0.001491 | Test acc: 0.89000\n",
      "Steps180 | Train loss: 0.001238 | Train acc: 0.91275 | Test loss: 0.001458 | Test acc: 0.89350\n",
      "Steps181 | Train loss: 0.001207 | Train acc: 0.91538 | Test loss: 0.001433 | Test acc: 0.89650\n",
      "Steps182 | Train loss: 0.001215 | Train acc: 0.91512 | Test loss: 0.001433 | Test acc: 0.89950\n",
      "Steps183 | Train loss: 0.001233 | Train acc: 0.91538 | Test loss: 0.001431 | Test acc: 0.89850\n",
      "Steps184 | Train loss: 0.001257 | Train acc: 0.91550 | Test loss: 0.001436 | Test acc: 0.89900\n",
      "Steps185 | Train loss: 0.001241 | Train acc: 0.91625 | Test loss: 0.001422 | Test acc: 0.89950\n",
      "Steps186 | Train loss: 0.001202 | Train acc: 0.91663 | Test loss: 0.001409 | Test acc: 0.90050\n",
      "Steps187 | Train loss: 0.001218 | Train acc: 0.91675 | Test loss: 0.001407 | Test acc: 0.89950\n",
      "Steps188 | Train loss: 0.001220 | Train acc: 0.91725 | Test loss: 0.001406 | Test acc: 0.90000\n",
      "Steps189 | Train loss: 0.001190 | Train acc: 0.91800 | Test loss: 0.001397 | Test acc: 0.90150\n",
      "Steps190 | Train loss: 0.001183 | Train acc: 0.91912 | Test loss: 0.001384 | Test acc: 0.90000\n",
      "Steps191 | Train loss: 0.001172 | Train acc: 0.91863 | Test loss: 0.001386 | Test acc: 0.90050\n",
      "Steps192 | Train loss: 0.001151 | Train acc: 0.92012 | Test loss: 0.001382 | Test acc: 0.90250\n",
      "Steps193 | Train loss: 0.001165 | Train acc: 0.92100 | Test loss: 0.001390 | Test acc: 0.90250\n",
      "Steps194 | Train loss: 0.001171 | Train acc: 0.92175 | Test loss: 0.001411 | Test acc: 0.89950\n",
      "Steps195 | Train loss: 0.001187 | Train acc: 0.91975 | Test loss: 0.001428 | Test acc: 0.89950\n",
      "Steps196 | Train loss: 0.001202 | Train acc: 0.91863 | Test loss: 0.001444 | Test acc: 0.90000\n",
      "Steps197 | Train loss: 0.001199 | Train acc: 0.91987 | Test loss: 0.001440 | Test acc: 0.89950\n",
      "Steps198 | Train loss: 0.001199 | Train acc: 0.92212 | Test loss: 0.001422 | Test acc: 0.90100\n",
      "Steps199 | Train loss: 0.001178 | Train acc: 0.92250 | Test loss: 0.001421 | Test acc: 0.90300\n",
      "Steps200 | Train loss: 0.001172 | Train acc: 0.92288 | Test loss: 0.001414 | Test acc: 0.90300\n",
      "Steps201 | Train loss: 0.001160 | Train acc: 0.92025 | Test loss: 0.001409 | Test acc: 0.90450\n",
      "Steps202 | Train loss: 0.001171 | Train acc: 0.91850 | Test loss: 0.001418 | Test acc: 0.90350\n",
      "Steps203 | Train loss: 0.001182 | Train acc: 0.91638 | Test loss: 0.001421 | Test acc: 0.90300\n",
      "Steps204 | Train loss: 0.001180 | Train acc: 0.91687 | Test loss: 0.001417 | Test acc: 0.90300\n",
      "Steps205 | Train loss: 0.001199 | Train acc: 0.91638 | Test loss: 0.001404 | Test acc: 0.90150\n",
      "Steps206 | Train loss: 0.001150 | Train acc: 0.91675 | Test loss: 0.001392 | Test acc: 0.89850\n",
      "Steps207 | Train loss: 0.001157 | Train acc: 0.92087 | Test loss: 0.001381 | Test acc: 0.90200\n",
      "Steps208 | Train loss: 0.001125 | Train acc: 0.92175 | Test loss: 0.001373 | Test acc: 0.90300\n",
      "Steps209 | Train loss: 0.001134 | Train acc: 0.92300 | Test loss: 0.001375 | Test acc: 0.90300\n",
      "Steps210 | Train loss: 0.001149 | Train acc: 0.92350 | Test loss: 0.001390 | Test acc: 0.90200\n",
      "Steps211 | Train loss: 0.001151 | Train acc: 0.92363 | Test loss: 0.001405 | Test acc: 0.90450\n",
      "Steps212 | Train loss: 0.001169 | Train acc: 0.92412 | Test loss: 0.001411 | Test acc: 0.90450\n",
      "Steps213 | Train loss: 0.001136 | Train acc: 0.92325 | Test loss: 0.001420 | Test acc: 0.90300\n",
      "Steps214 | Train loss: 0.001122 | Train acc: 0.92325 | Test loss: 0.001409 | Test acc: 0.89950\n",
      "Steps215 | Train loss: 0.001110 | Train acc: 0.92500 | Test loss: 0.001386 | Test acc: 0.90100\n",
      "Steps216 | Train loss: 0.001097 | Train acc: 0.92663 | Test loss: 0.001369 | Test acc: 0.90400\n",
      "Steps217 | Train loss: 0.001106 | Train acc: 0.92600 | Test loss: 0.001364 | Test acc: 0.90450\n",
      "Steps218 | Train loss: 0.001093 | Train acc: 0.92537 | Test loss: 0.001361 | Test acc: 0.90500\n",
      "Steps219 | Train loss: 0.001117 | Train acc: 0.92237 | Test loss: 0.001369 | Test acc: 0.90250\n",
      "Steps220 | Train loss: 0.001142 | Train acc: 0.91975 | Test loss: 0.001385 | Test acc: 0.90400\n",
      "Steps221 | Train loss: 0.001146 | Train acc: 0.91950 | Test loss: 0.001383 | Test acc: 0.90350\n",
      "Steps222 | Train loss: 0.001124 | Train acc: 0.92112 | Test loss: 0.001380 | Test acc: 0.90500\n",
      "Steps223 | Train loss: 0.001104 | Train acc: 0.92512 | Test loss: 0.001360 | Test acc: 0.90250\n",
      "Steps224 | Train loss: 0.001104 | Train acc: 0.92637 | Test loss: 0.001361 | Test acc: 0.90650\n",
      "Steps225 | Train loss: 0.001123 | Train acc: 0.92425 | Test loss: 0.001382 | Test acc: 0.90400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4507/1410544342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# trainer.train(model_function, config, balanced_global_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_patch_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ViT-Planetarium/src/vit_prisma/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_function, config, train_dataset, val_dataset, checkpoint_path)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Steps{steps} | Train loss: {train_loss:.6f} | Test loss: {test_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Steps{steps} | Train loss: {train_loss:.6f} | Train acc: {train_acc:.5f} | Test loss: {test_loss:.6f} | Test acc: {test_acc:.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ViT-Planetarium/src/vit_prisma/training/training_utils.py\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(net, data_loader, device, N, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import BaseViT\n",
    "from vit_prisma.training import trainer\n",
    "\n",
    "\n",
    "\n",
    "config = GlobalConfig()\n",
    "\n",
    "config\n",
    "\n",
    "model_function = BaseViT\n",
    "\n",
    "# trainer.train(model_function, config, balanced_global_dataset)\n",
    "\n",
    "trainer.train(model_function, config, local_patch_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.image.image_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
